<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Getting started with ellmer</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Getting started with ellmer</h1>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(ellmer)</span></code></pre></div>
<p>ellmer makes it easy to access the wealth of large language models
(LLMs) from R. But what can you do with those models once you have
access to them? This vignette will give you the basic vocabulary you
need to use an LLM effectively and will show you some examples to ignite
your creativity.</p>
<p>In this vignette we’ll mostly ignore how LLMs work, using them as
convenient black boxes. If you want to get a sense of how they actually
work, we recommend watching Jeremy Howard’s posit::conf(2023) keynote:
<a href="https://www.youtube.com/watch?v=sYliwvml9Es">A hacker’s guide
to open source LLMs</a>.</p>
<div id="vocabulary" class="section level2">
<h2>Vocabulary</h2>
<p>We’ll start by laying out the key vocab that you’ll need to
understand LLMs. Unfortunately the vocab is all a little entangled: to
understand one term you’ll often have to know a little about some of the
others. So we’ll start with some simple definitions of the most
important terms then iteratively go a little deeper.</p>
<p>It all starts with a <strong>prompt</strong>, which is the text
(typically a question or a request) that you send to the LLM. This
starts a <strong>conversation</strong>, a sequence of turns that
alternate between user prompts and model responses. Inside the model,
both the prompt and response are represented by a sequence of
<strong>tokens</strong>, which represent either individual words or
subcomponents of a word. The tokens are used to compute the cost of
using a model and to measure the size of the <strong>context</strong>,
the combination of the current prompt and any previous prompts and
responses used to generate the next response.</p>
<p>It’s useful to make the distinction between providers and models. A
<strong>provider</strong> is a web API that gives access to one or more
<strong>models</strong>. The distinction is a bit subtle because
providers are often synonymous with a model, like OpenAI and GPT,
Anthropic and Claude, and Google and Gemini. But other providers, like
Ollama, can host many different models, typically open source models
like LLaMa and Mistral. Still other providers support both open and
closed models, typically by partnering with a company that provides a
popular closed model. For example, Azure OpenAI offers both open source
models and OpenAI’s GPT, while AWS Bedrock offers both open source
models and Anthropic’s Claude.</p>
<div id="what-is-a-token" class="section level3">
<h3>What is a token?</h3>
<p>An LLM is a <em>model</em>, and like all models needs some way to
represent its inputs numerically. For LLMs, that means we need some way
to convert words to numbers. This is the goal of the
<strong>tokenizer</strong>. For example, using the GPT 4o tokenizer, the
string “When was R created?” is converted to 5 tokens: 5958 (“When”),
673 (” was”), 460 (” R”), 5371 (” created”), 30 (“?”). As you can see,
many simple strings can be represented by a single token. But more
complex strings require multiple tokens. For example, the string
“counterrevolutionary” requires 4 tokens: 32128 (“counter”), 264 (“re”),
9477 (“volution”), 815 (“ary”). (You can see how various strings are
tokenized at <a href="http://tiktokenizer.vercel.app/" class="uri">http://tiktokenizer.vercel.app/</a>).</p>
<p>It’s important to have a rough sense of how text is converted to
tokens because tokens are used to determine the cost of a model and how
much context can be used to predict the next response. On average an
English word needs ~1.5 tokens so a page might require 375-400 tokens
and a complete book might require 75,000 to 150,000 tokens. Other
languages will typically require more tokens, because (in brief) LLMs
are trained on data from the internet, which is primarily in
English.</p>
<p>LLMs are priced per million tokens. State of the art models (like
GPT-4.1 or Claude 3.5 sonnet) cost $2-3 per million input tokens, and
$10-15 per million output tokens. Cheaper models can cost much less,
e.g. GPT-4.1 nanoo costs $0.10 per million input tokens and $0.40 per
million output tokens. Even $10 of API credit will give you a lot of
room for experimentation, particularly with cheaper models, and prices
are likely to decline as model performance improves.</p>
<p>Tokens also used to measure the context window, which is how much
text the LLM can use to generate the next response. As we’ll discuss
shortly, the context length includes the full state of your conversation
so far (both your prompts and the model’s responses), which means that
cost grow rapidly with the number of conversational turns.</p>
<p>In ellmer, you can see how many tokens a conversations has used by
printing it, and you can see total usage for a session with
<code>token_usage()</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>chat <span class="ot">&lt;-</span> <span class="fu">chat_openai</span>(<span class="at">model =</span> <span class="st">&quot;gpt-4.1&quot;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>. <span class="ot">&lt;-</span> chat<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Who created R?&quot;</span>, <span class="at">echo =</span> <span class="cn">FALSE</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>chat</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">#&gt; &lt;Chat OpenAI/gpt-4.1 turns=2 tokens=11/92 $0.00&gt;</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">#&gt; ── user [11] ───────────────────────────────────────────────────────────────────</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co">#&gt; Who created R?</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co">#&gt; ── assistant [92] ──────────────────────────────────────────────────────────────</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co">#&gt; **R** is a programming language and software environment for statistical computing and graphics. It was **created by Ross Ihaka and Robert Gentleman** at the University of Auckland, New Zealand, in the early 1990s. The project started in 1992, and the first public release was made in 1995. Since then, R has grown into a large, collaborative open-source project supported by the R Development Core Team and a vibrant global community.</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="fu">token_usage</span>()</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co">#&gt;   provider   model input output price</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co">#&gt; 1   OpenAI gpt-4.1    11     92 $0.00</span></span></code></pre></div>
<p>If you want to learn more about tokens and tokenizers, I’d recommend
watching the first 20-30 minutes of <a href="https://www.youtube.com/watch?v=zduSFxRajkE">Let’s build the GPT
Tokenizer</a> by Andrej Karpathy. You certainly don’t need to learn how
to build your own tokenizer, but the intro will give you a bunch of
useful background knowledge that will help improve your undersstanding
of how LLM’s work.</p>
</div>
<div id="what-is-a-conversation" class="section level3">
<h3>What is a conversation?</h3>
<p>A conversation with an LLM takes place through a series of HTTP
requests and responses: you send your question to the LLM as an HTTP
request, and it sends back its reply as an HTTP response. In other
words, a conversation consists of a sequence of a paired turns: a sent
prompt and a returned response.</p>
<p>It’s important to note that a request includes not only the current
user prompt, but every previous user prompt and model response. This
means that:</p>
<ul>
<li><p>The cost of a conversation grows quadratically with the number of
turns: if you want to save money, keep your conversations
short.</p></li>
<li><p>Each response is affected by all previous prompts and responses.
This can make a converstion get stuck in a local optimum, so it’s
generally better to iterate by starting a new conversation with a better
prompt rather than having a long back-and-forth.</p></li>
<li><p>ellmer has full control over the conversational history. Because
it’s ellmer’s responsibility to send the previous turns of the
conversation, it’s possible to start a conversation with one model and
finish it with another.</p></li>
</ul>
</div>
<div id="what-is-a-prompt" class="section level3">
<h3>What is a prompt?</h3>
<p>The user prompt is the question that you send to the model. There are
two other important prompts that underlie the user prompt:</p>
<ul>
<li><p>The <strong>platform prompt</strong>, which is unchangeable, set
by the model provider, and affects every conversation. You can see what
these look like from Anthropic, who <a href="https://docs.anthropic.com/en/release-notes/system-prompts">publishes
their core system prompts</a>.</p></li>
<li><p>The <strong>system prompt</strong> (aka developer prompt), which
is set when you create a new conversation, and affects every response.
It’s used to provide additional instructions to the model, shaping its
responses to your needs. For example, you might use the system prompt to
ask the model to always respond in Spanish or to write dependency-free
base R code. You can also use the system prompt to provide the model
with information it wouldn’t otherwise know, like the details of your
database schema, or your preferred ggplot2 theme and color
palette.</p></li>
</ul>
<p>OpenAI calls this the <a href="https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command">chain
of command</a>: if there are conflicts or inconsistencies the prompts,
the platform prompt overrides the system prompt, which in turn overrides
the user prompt.</p>
<p>When you use a chat app like ChatGPT or claude.ai you can only
iterate on the user prompt. But when you’re programming with LLMs,
you’ll primarily iterate on the system prompt. For example, if you’re
developing an app that helps a user write tidyverse code, you’d work
with the system prompt to ensure that user gets the style of code they
want.</p>
<p>Writing a good prompt, which is called <strong>prompt
design</strong>, is key to effective use of LLMs. It is discussed in
more detail in <code>vignette(&quot;prompt-design&quot;)</code>.</p>
</div>
</div>
<div id="example-uses" class="section level2">
<h2>Example uses</h2>
<p>Now that you’ve got the basic vocab under your belt, I’m going to
fire a bunch of interesting potential use cases at you. While there are
special purpose tools that might solve these cases faster and/or
cheaper, an LLM allows you to rapidly prototype a solution. This can be
extremely valuable even if you end up using those more specialised tools
in your final product.</p>
<p>In general, we recommend avoiding LLMs where accuracy is critical.
That said, there are still many cases for their use. For example, even
though they always require some manual fiddling, you might save a bunch
of time ever with an 80% correct solution. In fact, even a not-so-good
solution can still be useful because it makes it easier to get started:
it’s easier to react to something rather than to have to start from
scratch with a blank page.</p>
<div id="chatbots" class="section level3">
<h3>Chatbots</h3>
<p>A great place to start with ellmer and LLMs is to build a chatbot
with a custom prompt. Chatbots are a familiar interface to LLMs and are
easy to create in R with <a href="https://github.com/jcheng5/shinychat">shinychat</a>. And there’s a
surprising amount of value to creating a custom chatbot that has a
prompt stuffed with useful knowledge. For example:</p>
<ul>
<li><p>Help people use your new package. To do so, you need a custom
prompt because LLMs were trained on data prior to your package’s
existence. You can create a surprisingly useful tool just by preloading
the prompt with your README and vignettes. This is how the <a href="https://github.com/jcheng5/ellmer-assistant">ellmer assistant</a>
works.</p></li>
<li><p>Build language specific prompts for R and/or python. <a href="https://shiny.posit.co/blog/posts/shiny-assistant/">Shiny
assistant</a> helps you build shiny apps (either in R or python) by
combining a <a href="https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt.md">prompt</a>
that gives general advice on building apps with a prompt for <a href="https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_r.md">R</a>
or <a href="https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md">python</a>.
The python prompt is very detailed because there’s much less information
about Shiny for Python in the existing LLM knowledgebases.</p></li>
<li><p>Help people find the answers to their questions. Even if you’ve
written a bunch of documentation for something, you might find that you
still get questions because folks can’t easily find exactly what they’re
looking for. You can reduce the need to answer these questions by
creating a chatbot with a prompt that contains your documentation. For
example, if you’re a teacher, you could create a chatbot that includes
your syllabus in the prompt. This eliminates a common class of question
where the data necessary to answer the question is available, but hard
to find.</p></li>
</ul>
<p>Another direction is to give the chatbot additional context about
your current environment. For example, <a href="https://github.com/cpsievert/aidea">aidea</a> allows the user to
interactively explore a dataset with the help of the LLM. It adds
summary statistics about the dataset to the <a href="https://github.com/cpsievert/aidea/blob/main/inst/app/prompt.md">prompt</a>
so that the LLM knows something about your data. Along these lines,
imagine writing a chatbot to help with data import that has a prompt
which include all the files in the current directory along with their
first few lines.</p>
</div>
<div id="structured-data-extraction" class="section level3">
<h3>Structured data extraction</h3>
<p>LLMs are often very good at extracting structured data from
unstructured text. This can give you traction to analyse data that was
previously unaccessible. For example:</p>
<ul>
<li><p>Customer tickets and GitHub issues: you can use LLMs for quick
and dirty sentiment analysis by extracting any specifically mentioned
products and summarising the discussion as a few bullet points.</p></li>
<li><p>Geocoding: LLMs do a surprisingly good job at geocoding,
especially extracting addresses or finding the latitute/longitude of
cities. There are specialised tools that do this better, but using an
LLM makes it easy to get started.</p></li>
<li><p>Recipes: I’ve extracted structured data from baking and cocktail
recipes. Once you have the data in a structured form you can use your R
skills to better understand how recipes vary within a cookbook or to
look for recipes that use the ingredients currently in your kitchen. You
could even use shiny assistant to help make those techniques available
to anyone, not just R users.</p></li>
</ul>
<p>Structured data extraction also works well with images. It’s not the
fastest or cheapest way to extract data but it makes it really easy to
prototype ideas. For example, maybe you have a bunch of scanned
documents that you want to index. You can convert PDFs to images
(e.g. using {imagemagick}) then use structured data extraction to pull
out key details.</p>
<p>Learn more about structured data extraction in
<code>vignette(&quot;structure-data&quot;)</code>.</p>
</div>
<div id="programming" class="section level3">
<h3>Programming</h3>
<p>LLMs can also be useful to solve general programming problems. For
example:</p>
<ul>
<li><p>Write a detailed prompt that explains how to update code to use a
new version of a package. You could combine this with the rstudioapi
package to allow the user to select code, transform it, and replace it
in the existing text. A comprehensive example of this sort of app is <a href="https://simonpcouch.github.io/chores/">chores</a>, which includes
prompts for automatically generating roxygen documentation blocks,
updating testthat code to the 3rd edition, and converting
<code>stop()</code> and <code>abort()</code> to use
<code>cli::cli_abort()</code>.</p></li>
<li><p>You could automatically look up the documentation for an R
function, and include it in the prompt to make it easier to figure out
how to use a specific function.</p></li>
<li><p>You can use LLMs to explain code, or even ask them to <a href="https://bsky.app/profile/daviddiviny.com/post/3lb6kjaen4c2u">generate
a diagram</a>.</p></li>
<li><p>You can ask an LLM to analyse your code for potential code smells
or security issues. You can do this a function at a time, or explore the
entire source code of your package or script in the prompt.</p></li>
<li><p>You could use <a href="https://gh.r-lib.org">gh</a> to find
unlabelled issues, extract the text, and ask the LLM to figure out what
labels might be most appropriate. Or maybe an LLM might be able to help
people create better reprexes, or simplify reprexes that are too
complicated?</p></li>
<li><p>I find it useful to have an LLM document a function for me, even
knowing that it’s likely to be mostly incorrect. Having something to
react to make it much easier for me to get started.</p></li>
<li><p>If you’re working with code or data from another programming
language, you can ask an LLM to convert it to R code for you. Even if
it’s not perfect, it’s still typically much faster than doing everything
yourself.</p></li>
</ul>
</div>
</div>
<div id="miscellaneous" class="section level2">
<h2>Miscellaneous</h2>
<p>To finish up here are a few other ideas that seem cool but didn’t
seem to fit the above categories:</p>
<ul>
<li><p>Automatically generate alt text for plots, using
<code>content_image_plot()</code>.</p></li>
<li><p>Analyse the text of your statistical report to look for flaws in
your statistical reasoning (e.g. misinterpreting p-values or assuming
causation where only correlation exists).</p></li>
<li><p>Use your existing company style guide to generate a <a href="https://posit-dev.github.io/brand-yml/articles/llm-brand-yml-prompt/">brand.yaml</a>
specification to automatically style your reports, apps, dashboards and
plots to match your corporate style guide.</p></li>
</ul>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
